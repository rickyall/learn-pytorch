{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- [深度强化学习 ( DQN ) 初探](https://www.qcloud.com/community/article/549802?fromSource=gwzcw.114127.114127.114127)\n",
    "- [DQN 从入门到放弃1 DQN与增强学习](https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit)\n",
    "- [DQN 从入门到放弃2 增强学习与MDP](https://zhuanlan.zhihu.com/p/21292697?refer=intelligentunit)\n",
    "- [DQN 从入门到放弃3 价值函数与Bellman方程](https://zhuanlan.zhihu.com/p/21340755?refer=intelligentunit)\n",
    "- [DQN 从入门到放弃4 动态规划与Q-Learning](https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit)\n",
    "- [DQN从入门到放弃5 深度解读DQN算法](https://zhuanlan.zhihu.com/p/21421729)\n",
    "- [Batch Normalization学习笔记及其实现](https://zhuanlan.zhihu.com/p/26138673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\",{\"font.sans-serif\":['simhei', 'Arial']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 前言\n",
    "本案例通过采用DQN模型来训练一个AI玩CartPole-v0的游戏。\n",
    "### 强化学习算法\n",
    "强化学习强调如何基于环境而行动，以取得最大化的预期利益，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。\n",
    "\n",
    "在强化学习的世界里，算法称之为Agent，与环境发生交互，Agent从环境中获取状态state，并决定自己要做出的动作action，环境会根据自身的逻辑给Agent予以奖励（reward）。有正向和反向的激励。\n",
    "\n",
    "### 马尔科夫决策过程\n",
    "环境是随机的，AI可以在环境中做出某些特定的动作，行为可以改变环境，并带来新的状态，代理可以再执行下一个动作。我们在选择这些动作的规则就叫做策略。\n",
    "\n",
    "状态与动作的集合加上改变状态的规则，就组成了一个马尔科夫决策过程。这个过程的一个情节（episode）形成了状态、动作与奖励的有限序列。\n",
    "> $$s_0, a_0, r_1, s_1, a_1, r_2, s_2, ... ,s_{n-1}, a_{n-1}, r_n, s_n$$\n",
    "\n",
    "\n",
    "### 策略policy\n",
    "状态State和动作Action存在映射关系，也就是一个state对应一个action，或者对应不同的概率（概率最高的就是最值得执行的动作）。状态与动作的关系其实就是输入与输出关系，而状态State到动作Action的过程就称之为一个策略Policy，一般用$\\pi$来表示，需要找到以下关系：\n",
    "$$a=\\pi(s)或\\pi(a|s)$$\n",
    "> 增强学习的任务就是找到一个最优的策略policy从而使reward最多。\n",
    "\n",
    "\n",
    "### 折扣未来的奖励\n",
    "我们在决定动作的时候，不仅要考虑即时的奖励，也需要考虑得到的未来的奖励。对于给定的马尔科夫决策过程的一次运行，我们可以很容易的计算一个情节的总奖励：\n",
    "$$R = r_1 + r_2 + r_3 + ... + r_n$$ \n",
    "因此，时间t的未来总回报为：\n",
    "$$R_t = r_t + r_{t+1} + r_{t+2} + r_{t+3}+...+r_{t+n}$$\n",
    "由于环境是随机的，无法确定相同的动作得到一样的奖励，因此需要利用折扣未来奖励来代替：\n",
    "$$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^{n-t}r_n$$\n",
    "在这里$\\gamma$为贴现因子，数值在0，1之间。用递归的方式表示：\n",
    "$$R_t = r_t + \\gamma(r_{t+1} + \\gamma(r_{t+2} +...) = r_t + \\gamma R_{t+1}$$\n",
    "因此，如果我们不考虑未来的收益，那么$\\gamma$应该为0，如果认为未来的环境是确定的，相同的动作导致相同的奖励，可以将贴现因子定义为$r=1$，如果希望是平衡的思路，那么$\\gamma=0.9$。\n",
    "> 在具体的投资过程中，市场的风格是经常多变的，所有我们的折扣因子不能为1，当然也不能为0，短期内假定市场风格不会剧烈的变动，那么设置一个偏向于短期的$\\gamma$似乎是比较合理的。\n",
    "\n",
    "### Value Function价值函数\n",
    "我们做决策时有目的的，那就是为了最大化未来的回报Result。根据某个策略和当前状态，我们选择了某一个动作，这个动作使得未来的回报最大。\n",
    "> if 某一个决策的价值最大：\n",
    "- 选择这个决策\n",
    "\n",
    "我们也可以使用策略+价值评估的方法联合给出决策，这种算法就是所谓的Actor-Critic算法。\n",
    "\n",
    "### Bellman方程\n",
    "一般我们基于以下几种情况来做评估：\n",
    "- 其他人的选择：看到其他人的投资成功率，我们会降低我们投资股票的价值。\n",
    "- 自己反复试验：我们通常不是只做一次选择，而是做了很多次选择，从而收获了所谓的“经验”的东西。我们根据经验来评估选择的价值。比如我们做了好几次投资楼市的选择，结果大获成功，因此我们就会认为投资楼市是不错的选择。\n",
    "- 基于理性分析：我们根据我们已有的知识对当前的情况做分析，从而做出一定的判断。\n",
    "- 基于感性的逻辑：比如选择投资自己到人工智能领域。\n",
    "\n",
    "计算机如何才能评估：\n",
    "- 其他人的选择：没有其他人\n",
    "- 基于理性分析：没有人类的先验知识，无法分析，可以先用监督学习然后再用增强学习。\n",
    "- 基于感性的逻辑：不行\n",
    "- 基于反复的试验：可行\n",
    "\n",
    "价值等于未来的期望回报\n",
    "$$v(s) = E[R_{t+1} + \\lambda v(S_{t+1})|S_t=s]$$\n",
    "上面的公式为Bellman方程的基本形态，从公示上看，当前状态的价值和下一步的价值以及当前的反馈Reward有关。\n",
    "\n",
    "### 动作价值函数\n",
    "如果在状态转移之间考虑动作，那么采取不同的动作，会产生不同的序列，结果获得的回报也不同，这表明，动作也是存在价值的。显然，如果知道了每个动作的价值，那么就可以选择价值最大的一个动作去执行。这就是Action-Value function $Q^{\\pi}(s, a)$。\n",
    "\n",
    "那么有了上面的定义，动作价值函数就为如下表示：\n",
    "$$Q^{\\pi}(s,a)=E[r_{t+1} + \\lambda r_{t+2} + \\lambda^2 r_{t+3}+...|s,a]\\\n",
    "= E_{s^{'}}[r + \\lambda Q^{\\pi}(s^{'}, a^{'})|s, a]$$\n",
    "\n",
    "### Optimal value function 最优价值函数\n",
    "能计算动作价值函数是不够的，因为我们需要的是最优策略，现在求解最优策略等价于求解最优的value function，找到了最优的value function，自然而然策略也就找到了。（DQN是value-base, 后面还有Policy-based, model-based）\n",
    "$$Q^{*}(s, a) = max_{\\pi}Q^{\\pi}(s,a)$$\n",
    "最优的动作价值函数为所有策略下的动作价值函数最大值。通过这样的定义就可以使最优的动作价值函数唯一。\n",
    "套用上一届得到的Value function，可以得到\n",
    "$$Q^{*}(s, a) = E_{s^{'}}[r + \\lambda max_{a^{'}}Q^{*}(s^{'}, a^{'})|s, a]$$\n",
    "因为最优的Q值必然为最大值，等式右侧的Q值必然为使$a^{'}$取最大的Q值。\n",
    "\n",
    "### 策略迭代 Policy Iteration\n",
    "目的是通过迭代计算value function价值函数的方式来使得policy收敛到最优。\n",
    "- policy evaluation策略评估。目的是更新value function,或者说更好的估计基于当前策略的价值。\n",
    "- policy improvement策略改进。使用greedy policy产生新的样本用于第一步的策略评估。\n",
    "\n",
    "本质上就是使用当前策略产生新的样本，然后使用新的样本更好的估计策略的价值，然后利用策略的价值更新策略，然后不断的反复。理论上可以证明最终的策略将收敛到最优。\n",
    "\n",
    "### Q-learning算法描述\n",
    "- 初始化Q(s, a)，设置Q(terminal-state,.) = 0\n",
    "- 重复（对每一节episode）：\n",
    "    - 初始化状态S\n",
    "    - 重复 （对episode中的每一步）：\n",
    "        - 使用某一个policy比如（e - greedy）根据状态S选取一个动作执行。\n",
    "        - 执行结束后观察reward和新的状态$S^{'}$\n",
    "        - $Q(S_t, A_t) <- Q(S_t, A_t) + \\alpha(R_{t+1} + \\lambda max_{a}Q(S_{t+1}, a)-Q(S_t, A_t))$\n",
    "        - S<-$S^{'}$\n",
    "    - 循环直到S终止\n",
    "\n",
    "算法中$\\alpha$是学习率，当$\\alpha$为1时，结果为贝尔曼方程。更新次数越多Q函数越接近真实的Q值。\n",
    "\n",
    "### Exploration and Exploiation探索与利用\n",
    "Q-learning不需要优化policy，因此是off-policy的算法，另一方面，因为Q-Learning完全不考虑Model模型也就是环境的具体情况，只考虑看到的环境及reward，因此是model-free的方法。（PS：比较适合金融市场）\n",
    "\n",
    "那么如何生成策略：\n",
    "- 随机生成一个动作\n",
    "- 根据当前Q值计算出一个最优的动作，这个policy $\\pi$ 称之为greedy policy贪婪策略，也就是\n",
    "$$\\pi(S_{t+1}) = argmax_a Q(S_{t+1}, a)$$\n",
    "\n",
    "使用随机的动作就是exploration，探索未知的动作会产生的效果，有利于更新Q值，而使用greedy policy也就是target policy则是exploitation，利用Policy，相对来说不好更新出更好的Q值，单可以得到更好的测试效果用于判断算法是否有效。将两者结合起来就是所谓的$\\xi - greedy$策略，$\\xi$一般是一个很小的值，作为选取随机动作的概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q-table更新案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 初始化Q-table\n",
    "q_table = pd.DataFrame(0, \n",
    "                       columns=['a1', 'a2', 'a3', 'a4'], \n",
    "                       index=['s1', 's2', 's3', 's4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "lambdas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 第一步在s1随机选择一个动作，得到的reward为1，并进入s3\n",
    "random_action = np.random.choice(['a1', 'a2', 'a3', 'a4'])\n",
    "reward = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 更新Q值\n",
    "q_table.loc['s1', random_action] = q_table.loc['s1', random_action] + alpha * (reward + lambdas * q_table.loc['s3'].max() - q_table.loc['s1', random_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a1  a2  a3  a4\n",
       "s1   0   1   0   0\n",
       "s2   0   0   0   0\n",
       "s3   0   0   0   0\n",
       "s4   0   0   0   0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q-network\n",
    "我们只需要对高纬状态进行降维，而不需要对动作也进行降维处理。\n",
    "$Q(s)\\approx f(s, w)$ 神经网络直接输出各个动作的Q值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 本案例描述\n",
    "#### 任务\n",
    "AI需要决定向左移动还是向右移动，来使得木棍保持稳定。\n",
    "\n",
    "#### 依赖包\n",
    "- 神经网络（torch.nn）\n",
    "- 优化器（torch.optim）\n",
    "- 自动求导（torch.autograd）\n",
    "- 游戏包（gym）\n",
    "- 可视化（torchvision）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 载入神经网络相关包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "from torch import FloatTensor, LongTensor, ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-25 16:44:39,878] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# 载入gym环境\n",
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 保存回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"保存一次交互\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = ReplayMemory(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 定义DQN网络\n",
    "### Batch Normalization层\n",
    "一般，如果模型的输入特征不相关且满足标准正态分布$N(0, 1)$时，模型的表现一般较好。在训练神经网络模型的时候，我们可以事先将特征去相关，并使得他们满足一个比较好的分布，这样，模型的第一层网络一般都会有一个比较好的输入特征，但是随着模型的层数加深，网络的非线性变换使得每一层结果变得相关了，且不再满足$N(0, 1)$分布，更糟糕的是，可能隐藏层的特征分布已经发生了偏离。为了解决这个问题，提出在层与层之间加入batch_normalization层，维护所有mini-batch数据的均值方差，最后利用样本的均值方差的无偏估计量用于测试时使用。\n",
    "\n",
    "加入BN后，使得模型的训练收敛速度更快，模型隐藏输出特征的分布更稳定，更利于模型的学习。\n",
    "### Variable\n",
    "每个Variable有两个变量：requires_grad和volatile.\n",
    "- requires_grad: 主要用于限制变量是否需要进行梯度下降。\n",
    "- volatile: 纯输入接口，不能进行梯度下降。\n",
    "\n",
    "### torch.cat\n",
    "拼接数据\n",
    "- torch.cat((x, x, x), 0)：纵向拼接\n",
    "- torch.cat((x, x, x), 1)：横向拼接\n",
    "\n",
    "### optim.step\n",
    "- 在确定好梯度之后，更新神经网络参数\n",
    "\n",
    "#### （1）Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # 一层卷积\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # 两层卷积\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # 三层卷积\n",
    "        return self.head(x.view(x.size(0), -1))  # 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### （2）从环境中提取输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "torchvision.transforms: 主要用于进行图像转换。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "resize = T.Compose([\n",
    "    T.ToPILImage(),  # 转换成为PILImage\n",
    "    T.Scale(40, interpolation=Image.CUBIC),  # 缩小或放大\n",
    "    T.ToTensor()  # 转换成为tensor, (H X W X C) in range(255)=> (C X H X W) in range(1.0)\n",
    "])\n",
    "\n",
    "# 定义环境\n",
    "screen_width = 600\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # 转化成torch序列（CHW）\n",
    "    screen = screen[:, 169:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # 将图像转化成为tensor\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # 调整尺寸\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAEBCAYAAAA9wX86AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119e93668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 显示其中一次案例\n",
    "env.reset()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "plt.title('一次游戏截取案例');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 训练网络\n",
    "#### （1）初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "model = DQN()  # 初始化对象\n",
    "optimizer = optim.RMSprop(model.parameters())  # 设置优化器\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \"\"\"选择动作\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold: # 刚开始不采用DQN进行更新，采用随机探索\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)  # 返回最优的动作\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "episode_durations = []  # 维持时间长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('训练中。。。')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 平均没100次迭代画出一幅图\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))  # 拼接数据\n",
    "        plt.plot(means.numpy())\n",
    "    \n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### (2) 设计训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "last_sync = 0\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"训练函数\n",
    "    \"\"\"\n",
    "    global last_sync\n",
    "    if len(memory) < BATCH_SIZE: # 如果样本数小于最低批次大小返回 \n",
    "        return\n",
    "    # 转化batch\n",
    "    transitions = memory.sample(BATCH_SIZE)  # 抽样\n",
    "    batch = Transition(*zip(*transitions))  # 转换成为一批次\n",
    "    \n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]), volatile=True)\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "    \n",
    "    # 计算Q(s_t, a)，选择动作\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # 计算下一步的所有动作的价值V（s_{t+1}）\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    next_state_values.volatile = False\n",
    "    # 计算预期的Q值\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    # 计算Huber loss，损失函数采用smooth_ll_loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    # 优化模型\n",
    "    optimizer.zero_grad()  # 清理所有参数的梯度。\n",
    "    loss.backward()  # 反向传播\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)  # 将所有的梯度限制在-1到1之间\n",
    "    optimizer.step()  # 更新模型的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 100  # 迭代次数\n",
    "for i_episode in range(num_episodes):\n",
    "    # 初始化环境和状态\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen  # 定义状态，即为当前状态和最后的状态差。\n",
    "    for t in count():\n",
    "        # 选择并执行一个动作\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action[0, 0])  # 从环境汇中获取奖励\n",
    "        reward = Tensor([reward])  # 将奖励转换成为tensor\n",
    "        \n",
    "        # 观察新的状态，确定下一个状态（PS：在这一步里面获取了未来信息，引用在资本市场上，未来的状态具有一定的概率分布特征。）\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        # 将转换保存起来\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # 切换到下一状态\n",
    "        state = next_state\n",
    "        \n",
    "        # 优化模型\n",
    "        optimize_model()\n",
    "        # 一次游戏结束，就画图显示\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "print('完成')\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
